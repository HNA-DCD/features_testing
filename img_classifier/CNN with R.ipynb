{"cells":[{"metadata":{},"cell_type":"markdown","source":"# CNN for Intel Image Classification\n\nIn this project we're gonna build cnn model for image classification to distinct places such as buildings, forest, glacier, mountain, sea, and street. This model can be used for application that used landscape picture as its own features for instance to cluster recommendation places that similar with user input."},{"metadata":{},"cell_type":"markdown","source":"# Import Packages\n\nFirstly, we need to import several packages but mostly we just need packages for data manipulation and build deep learning architecture model. In this case because we just want to build simple cnn model, we can use keras that more user friendly."},{"metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","trusted":true},"cell_type":"code","source":"library(keras)\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(imager)","execution_count":2,"outputs":[{"output_type":"stream","text":"── \u001b[1mAttaching packages\u001b[22m ─────────────────────────────────────── tidyverse 1.2.1 ──\n\n\u001b[32m✔\u001b[39m \u001b[34mggplot2\u001b[39m 3.2.1.\u001b[31m9000\u001b[39m     \u001b[32m✔\u001b[39m \u001b[34mpurrr  \u001b[39m 0.3.2     \n\u001b[32m✔\u001b[39m \u001b[34mtibble \u001b[39m 2.1.3          \u001b[32m✔\u001b[39m \u001b[34mdplyr  \u001b[39m 0.8.3     \n\u001b[32m✔\u001b[39m \u001b[34mtidyr  \u001b[39m 0.8.3          \u001b[32m✔\u001b[39m \u001b[34mstringr\u001b[39m 1.4.0     \n\u001b[32m✔\u001b[39m \u001b[34mreadr  \u001b[39m 1.3.1          \u001b[32m✔\u001b[39m \u001b[34mforcats\u001b[39m 0.4.0     \n\n── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n\nLoading required package: magrittr\n\n\nAttaching package: ‘magrittr’\n\n\nThe following object is masked from ‘package:purrr’:\n\n    set_names\n\n\nThe following object is masked from ‘package:tidyr’:\n\n    extract\n\n\n\nAttaching package: ‘imager’\n\n\nThe following object is masked from ‘package:magrittr’:\n\n    add\n\n\nThe following object is masked from ‘package:stringr’:\n\n    boundary\n\n\nThe following object is masked from ‘package:tidyr’:\n\n    fill\n\n\nThe following objects are masked from ‘package:stats’:\n\n    convolve, spectrum\n\n\nThe following object is masked from ‘package:graphics’:\n\n    frame\n\n\nThe following object is masked from ‘package:base’:\n\n    save.image\n\n\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"## Set GPU Environment\n\nIn order to get fast computing, we can't use CPU processor to process our data because the size is too large. So kaggle provides gpu for us to compute our processes. In this section we can also check list of cores that are provided by kaggle."},{"metadata":{"trusted":true},"cell_type":"code","source":"Sys.setenv(KERAS_BACKEND = \"keras\")\nSys.setenv(THEANO_FLAGS = \"device=gpu,floatX=float32\")\nk = backend()\nsess = k$get_session()\nsess$list_devices()\nsystem(\"ls ../input\")","execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"[[1]]\n_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 6645422030548007246)\n\n[[2]]\n_DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 11911082925581861493)\n\n[[3]]\n_DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 6225206460837553550)\n\n[[4]]\n_DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:0, GPU, 15956161332, 7707700862761205860)\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Dataset Preparation\n\nFirstly, before we fit our data into CNN model, we have to serve it as a matrix form. Our images will be converted into 3 Dimensional Matrix (width, height, and channel). In our dataset folder, there are 3 subfolders contain train, test, and prediction dataset. In this case we're only use 2 subfolders, seg_train for making train set and validation set, and seg_test to evaluate our model.\n\nIn this section we're gonna do 3 main process for our images:\n- Data Transformation\n- Data Augmentation\n- Data Generator"},{"metadata":{},"cell_type":"markdown","source":"## Data Transormation\n\nBasically in this section, we will transform all of images in dataset folders into multidimensional matrix. So firstly we have to get our list images each of class respectively using list.files."},{"metadata":{"trusted":true},"cell_type":"code","source":"buildings <- list.files(path = \"../input/intel-image-classification/seg_train/seg_train/buildings\")\nforest <- list.files(path = \"../input/intel-image-classification/seg_train/seg_train/forest\")\nglacier <- list.files(path = \"../input/intel-image-classification/seg_train/seg_train/glacier\")\nmountain <- list.files(path = \"../input/intel-image-classification/seg_train/seg_train/mountain\")\nsea <- list.files(path = \"../input/intel-image-classification/seg_train/seg_train/sea\")\nstreet <- list.files(path = \"../input/intel-image-classification/seg_train/seg_train/street\")\n\nbuildings_test <- list.files(path = \"../input/intel-image-classification/seg_test/seg_test/buildings\")\nforest_test <- list.files(path = \"../input/intel-image-classification/seg_test/seg_test/forest\")\nglacier_test <- list.files(path = \"../input/intel-image-classification/seg_test/seg_test/glacier\")\nmountain_test <- list.files(path = \"../input/intel-image-classification/seg_test/seg_test/mountain\")\nsea_test <- list.files(path = \"../input/intel-image-classification/seg_test/seg_test/sea\")\nstreet_test <- list.files(path = \"../input/intel-image-classification/seg_test/seg_test/street\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After we read all of images from our folder, we make train, evaluation, and test based on them. Disclaimer, if we use all of our images for fitting model, it will take expensive computation and it doesn't guarantee our model score better. So we decide to not use all of them. Moreover we will do data augmentation so that we get more variety of data."},{"metadata":{"trusted":true},"cell_type":"code","source":"size = 150\nchannels = 3\n\ntrain <- c(\n    buildings[1:1500], \n    forest[1:1500],\n    glacier[1:1500],\n    mountain[1:1500],\n    sea[1:1500],\n    street[1:1500]\n)\ntrain <- sample(train)\nevaluation <- c(\n    buildings[1501:1750], \n    forest[1501:1750],\n    glacier[1501:1750],\n    mountain[1501:1750],\n    sea[1501:1750],\n    street[1501:1750]\n)\nevaluation <- sample(evaluation)\n\ntest <- c(buildings_test, forest_test, glacier_test, mountain_test, sea_test, street_test)\ntest <- sample(test)","execution_count":4,"outputs":[{"output_type":"error","ename":"ERROR","evalue":"Error in eval(expr, envir, enclos): object 'buildings' not found\n","traceback":["Error in eval(expr, envir, enclos): object 'buildings' not found\nTraceback:\n"]}]},{"metadata":{},"cell_type":"markdown","source":"After we get sample list of images, we have to make our data into matrix form both for X and y (label). For target or label, we can make new matrix like we do one hot encoding each of target class in our images. For X or variables, we prepare 4D tensor with shape detail total data, width, height, and channel (using 3 channel because RGB format). After our data transform into matrix, we can remove several variables that won't be used again for next process or clean up our environment (because it cost more memory usage if we don't remove its)."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_prep <- function(images, size, channels, path, list_img){\n\n  count<- length(images)\n  master_array <- array(NA, dim=c(count,size, size, channels))\n  \n  for (i in seq(length(images))) {\n    folder_list <- list(\"buildings\", \"forest\", \"glacier\", \"mountain\", \"sea\", \"street\")\n    for(j in 1:length(folder_list)) {\n        if(images[i] %in% list_img[[j]]) {\n            img_path <- paste0(path, folder_list[[j]], \"/\", images[i])\n            break\n        }\n    }\n    img <- image_load(path = img_path, target_size = c(size,size))\n    img_arr <- image_to_array(img)\n#     img <- load.image(paste(\"../input/train/\", images[i], sep=\"\"))\n#     img <-  resize(img,size_x = size, size_y = size, size_c = channels)\n#     img_arr <- array_reshape(img, c(1, size, size, channels))\n#     img_arr <- image_to_array(img)\n    img_arr <- array_reshape(img_arr, c(1, size, size, channels))\n    master_array[i,,,] <- img_arr\n  }\n  return(master_array)\n}\n\nlabel_prep <- function(images, list_img) {\n    y <- c()\n    for(i in seq(length(images))) {\n        folder_list <- list(\"buildings\", \"forest\", \"glacier\", \"mountain\", \"sea\", \"street\")\n        for(j in 1:length(folder_list)) {\n            if(images[i] %in% list_img[[j]]) {\n                y <- append(y, j-1)\n                break\n            }\n        }\n    }\n    return(y)\n}\n\nlist_img_train <- list(buildings, forest, glacier, mountain, sea, street)\nlist_img_test <- list(buildings_test, forest_test, glacier_test, mountain_test, sea_test, street_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train <- data_prep(train, size, channels, \"../input/intel-image-classification/seg_train/seg_train/\", list_img_train)\nX_evaluation <- data_prep(evaluation, size, channels, \"../input/intel-image-classification/seg_train/seg_train/\", list_img_train)\nX_test <- data_prep(test, size, channels, \"../input/intel-image-classification/seg_test/seg_test/\", list_img_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train <- to_categorical(label_prep(train, list_img_train))\ny_evaluation <- to_categorical(label_prep(evaluation, list_img_train))\ny_test <- to_categorical(label_prep(test, list_img_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rm(list_img_train, list_img_test, buildings, forest, glacier, mountain, sea, street, buildings_test, forest_test, glacier_test, mountain_test, sea_test, street_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Augmentation\n\nOne of most common problem with image classification is lack of datasets. Data augmentation helps us to get more images by manipulation or processing each of images such as rotation, flip, scale, etc. However data augmentation also depends to our cases. For instance, if we deal with medical images, we forbid to produce more data with rotation or flipping our base images. In this case for landscape picture, we can augment several images like horizontal flip, rescale our images, and shifting our images. We get more information without add more data to our folders."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_datagen <- image_data_generator(rescale = 1/255,\n  width_shift_range = 0.2,\n  height_shift_range = 0.2,\n  shear_range = 0.2,\n  zoom_range = 0.2,\n  horizontal_flip = TRUE)   \n\nvalidation_datagen <- image_data_generator(rescale = 1/255)   \ntest_datagen <- image_data_generator(rescale = 1/255)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Generator\n\nOn the process above, we just make some data generator for our images but actually not add new data based on image transformation. Now we want to wrap our data with generator together using batch size 32. After this process below is executed, our data prepare to be fit into CNN Model. To clean up our environment, we want to remove unused variables to save some memory usage."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_generator <- flow_images_from_data(\n  x = X_train, \n  y = y_train,\n  generator = train_datagen,                                                                                       \n  batch_size = 32\n)\n\nvalidation_generator <- flow_images_from_data(\n  x = X_evaluation, \n  y = y_evaluation,\n  generator = validation_datagen,                                                                                       \n  batch_size = 32\n)\n\ntest_generator <- flow_images_from_data(\n    x = X_test,\n    y = y_test,\n    generator = test_datagen,\n    batch_size = 32\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rm(X_train, y_train, X_evaluation, y_evaluation)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# CNN Model Architecture\n\nAfter dataset has prepared already, now we get into modelling section. In this case we want to classify images using convolutional neural network architecture, one of most popular NN architecture for image data. Basically the main differences using normal NN and CNN is convolutional layer. If we use dense layer, our model will learn global pattern in their input feature space whereas convolutional layer learn local layer. For instance at the cat classifier, cnn model can find local pattern such as edges, textures, and so on. More specifically, cnn model (maybe) learn ears, eyes, and etc. Moreover the advantage of using convolutional layers is they learn translation invariant. It means model can recognize objects anywhere whether the object at the bottom left corner, at the center, and so on due to learn local pattern.\n\nMainly our model architecture consist 4 different layers:\n- convolutional layers\n- pooling layers (for downsampling features)\n- dense layers (for output classifier)\n- dropout (prevent model overfitting)\n\nOur deep learning problem is classification with multiclass problem. Therefore on the last dense layer for output, we use softmax activation function."},{"metadata":{"trusted":true},"cell_type":"code","source":"model <- keras_model_sequential() %>%\n  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = \"relu\",\n                input_shape = c(150, 150, 3)) %>%\n  layer_max_pooling_2d(pool_size = c(2, 2)) %>%\n  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = \"relu\") %>%\n  layer_max_pooling_2d(pool_size = c(2, 2)) %>%\n  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = \"relu\") %>%\n  layer_max_pooling_2d(pool_size = c(2, 2)) %>%\n  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = \"relu\") %>%\n  layer_max_pooling_2d(pool_size = c(2, 2)) %>%\n  layer_flatten() %>%\n  layer_dropout(0.5)  %>% \n  layer_dense(units = 512, activation = \"relu\") %>%\n  layer_dense(units = 6, activation = \"softmax\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Compiler\n\nNext, we choose our model lost function and optimzer in order to do backpropagation. We use categorical crossentropy as cost function because our domain problem is multiclass classification and we use adam optimizer."},{"metadata":{"trusted":true},"cell_type":"code","source":"model %>% compile(\n  loss = \"categorical_crossentropy\",\n  optimizer = optimizer_adam(lr = 1e-3),\n  metrics = c(\"accuracy\")\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Phase \n\nFinally, we ready to train our model with dataset. We use train generator for training set and validation generator for validation set that are prepared before. Set total epoch can a little bit tricky due to higher epoch doesn't guarantee our model will better. So if at n-epoch model accuracy plot don't rise sharply or we can say steady at the same value, we can stop the process. It useful to save more time and less computation because we cut some unnecessary process. To this technique, we can use early stopping callback. In the example below, it means, if accuracy doesn't increase after 5 epoch, it will stop immediately and get the latest model fit."},{"metadata":{"trusted":true},"cell_type":"code","source":"history <- model %>% fit_generator(\n  train_generator,\n  steps_per_epoch = 100,\n  epochs = 30,\n  validation_data = validation_generator,\n  validation_steps = 50,\n  callbacks = list(\n      callback_early_stopping(patience = 5, monitor = \"accuracy\", mode = \"max\")\n  )\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After fitting process done, we can plot our history to check the result of training model. According to plot below, our model get approximately 86% accuracy both for training and validation data. Therefore our model doensn't tend to be overfitting.Actually we can train more epoch but based on the graph, it looks like doesn't improve significantly instead using more memory to run it."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Save Model\n\nTraining CNN model took times and sometimes our notebook crash because limited core or memory consumption. To prevent that we can save our model. So if we want to predict another data, we can load our keras model without train new model."},{"metadata":{"trusted":true},"cell_type":"code","source":"dir.create(\"model\", showWarnings = FALSE)\nmodel %>% save_model_hdf5(\"./model/my_model.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"load_model <- load_model_hdf5(\"./model/my_model.h5\")\nload_model %>% summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Evaluation\n\nAfter CNN model is created, we evaluate it to prove our model can distinct well every input images such as buildings, forest, glacier, mountain, sea, and street using evaluate generator function. If we take a look from history plot above, our cnn model fit the data properly both for train set and validation set (it doesn't tend to be overfit model). Finally if we run cell below, we can get our model accuracy about 87% and its score is same with train and validation when the model was fit."},{"metadata":{"trusted":true},"cell_type":"code","source":"load_model %>% evaluate_generator(test_generator, steps = 32)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"}},"nbformat":4,"nbformat_minor":1}